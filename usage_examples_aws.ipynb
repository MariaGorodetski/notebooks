{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage examples AWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import sklearn\n",
    "import xgboost\n",
    "from sklearn import metrics\n",
    "from collections import Counter\n",
    "from time import gmtime, strftime    \n",
    "from sklearn import metrics\n",
    "\n",
    "import tarfile\n",
    "import pickle as pkl\n",
    "\n",
    "import os\n",
    "import boto3\n",
    "import re\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.tuner import IntegerParameter, CategoricalParameter, ContinuousParameter, HyperparameterTuner\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AWS client setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "role = get_execution_role()\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "container = get_image_uri(region, 'xgboost')\n",
    "smclient = boto3.Session().client('sagemaker')\n",
    "\n",
    "bucket = 'bucket_name'\n",
    "bucket_path = 'http://bucket_path.s3.amazonaws.com/'\n",
    "prefix = 'prefix_to_directory'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_location = <local_path>\n",
    "time =  strftime(\"%d-%H-%M-%S\", gmtime())\n",
    "location = 's3://{}/{}/{}'.format(bucket, prefix, time)\n",
    "\n",
    "if not os.path.exists(local_location): \n",
    "    os.makedirs(local_location)\n",
    "    \n",
    "\n",
    "train.to_csv(local_location + '/train.csv', header=False, index=False)\n",
    "valid.to_csv(local_location + '/valid.csv', header=False, index=False)\n",
    "test.to_csv(local_location + '/test.csv', header=False, index=False)\n",
    "\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, time, 'train/train.csv')).upload_file(local_location+'/train.csv') \n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, time, 'test/test.csv')).upload_file(local_location+'/test.csv')\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, time, 'valid/valid.csv')).upload_file(local_location+'/valid.csv')\n",
    "\n",
    "train_data_location = '{}/{}'.format(location, 'train')\n",
    "test_data_location = '{}/{}'.format(location,'test')\n",
    "valid_data_location = '{}/{}'.format(location, 'valid')\n",
    "s3_output_location = location\n",
    "\n",
    "s3_input_train = sagemaker.session.s3_input(train_data_location, content_type='text/csv')\n",
    "s3_input_test = sagemaker.session.s3_input(test_data_location, content_type='text/csv')\n",
    "s3_input_valid = sagemaker.session.s3_input(valid_data_location, content_type='text/csv')\n",
    "\n",
    "data_channels = {'train': s3_input_train, 'validation': s3_input_valid}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_xgboost_obj(data_channels, location, diag, scale_pos_weight, diag_perc, alpha, colsample_bytree, max_depth, min_child_weight, subsample, num_round):\n",
    "    \n",
    "    job_name = \"{}-{}-{}\".format(diag[:10], strftime(\"%d-%H-%M-%S\", gmtime()), scale_pos_weight).replace('_', '-').replace('.', '-')\n",
    "    xgb_model = sagemaker.estimator.Estimator(\n",
    "                                    container,\n",
    "                                     role, \n",
    "                                     train_instance_count=1,#5 \n",
    "                                     train_instance_type='ml.m4.xlarge',  ## remotee\n",
    "                                     train_volume_size = 5, \n",
    "                                     output_path=location,\n",
    "                                    train_use_spot_instances=True,\n",
    "                                    train_max_wait=3600,\n",
    "                                    train_max_run = 3000, \n",
    "                                     sagemaker_session=sagemaker.Session())\n",
    "    \n",
    "    xgb_model.set_hyperparameters(\n",
    "                            alpha=alpha,\n",
    "                            colsample_bytree = colsample_bytree, \n",
    "                            max_depth=max_depth, \n",
    "                            eta=0.2,\n",
    "                            gamma=4,\n",
    "                            min_child_weight=min_child_weight,\n",
    "                            subsample=subsample,\n",
    "                            silent=0,\n",
    "                            objective='binary:logistic',\n",
    "                            scale_pos_weight = scale_pos_weight,\n",
    "                            num_round=num_round,\n",
    "                            eval_metric='auc')\n",
    "    \n",
    "    xgb_model.fit(data_channels, job_name=job_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HyperparameterTuner Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(data_channels, location, scale_pos_weight=1):\n",
    "    \n",
    "    xgb_model = sagemaker.estimator.Estimator(\n",
    "                                    container,\n",
    "                                     role, \n",
    "                                     train_instance_count=1, \n",
    "                                     train_instance_type='ml.m4.4xlarge',#'ml.m4.xlarge', ## remote\n",
    "                                     train_volume_size = 5,\n",
    "                                     output_path=location,\n",
    "                                     sagemaker_session=sagemaker.Session())\n",
    "    \n",
    "    xgb_model.set_hyperparameters(\n",
    "                            #max_depth=2,\n",
    "                            eta=0.2,\n",
    "                            gamma=4,\n",
    "                            #min_child_weight=6,\n",
    "                            #subsample=0.8,\n",
    "                            silent=0,\n",
    "                            objective='binary:logistic',\n",
    "                            scale_pos_weight = scale_pos_weight,\n",
    "                            #num_round=100,\n",
    "                            eval_metric='auc')\n",
    "\n",
    "    objective_metric_name = 'validation:auc'\n",
    "\n",
    "    hyperparameter_ranges = {\n",
    "    'lambda': ContinuousParameter(0.01, 10),\n",
    "    'max_depth':IntegerParameter(3, 9),\n",
    "    'colsample_bytree':ContinuousParameter(0.01, 0.5),\n",
    "    'alpha':ContinuousParameter(0.01, 0.1),\n",
    "    'num_round': IntegerParameter(50, 2000),\n",
    "    'subsample':ContinuousParameter(0.6, 1),\n",
    "    'min_child_weight':ContinuousParameter(10, 100)\n",
    "    }\n",
    "    \n",
    "    tuner = HyperparameterTuner(\n",
    "        xgb_model,\n",
    "        objective_metric_name,\n",
    "        hyperparameter_ranges,\n",
    "        max_jobs=100,#20\n",
    "        max_parallel_jobs=3,#10\n",
    "        strategy='Random'\n",
    "    )\n",
    "    \n",
    "    job_name = \"{}\".format(strftime(\"%d-%H-%M-%S\", gmtime())).replace('_', '-')\n",
    "    \n",
    "    tuner.fit(data_channels, include_cls_metadata=False, job_name=job_name, wait=True)\n",
    "\n",
    "    return xgb_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some utils functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_json_dump(bucket, dir_path, collection, timestamp, incremental_id, json_dump, log=None):\n",
    "    try:\n",
    "\n",
    "        create_bucket_directory(bucket, dir_path)\n",
    "        log.info('processed {} records on {}'.format(incremental_id, timestamp))\n",
    "        file_name = '{}-{}-{}.json'.format(collection, timestamp, incremental_id)\n",
    "        upload_to_bucket(bucket, dir_path, file_name, json_dump)\n",
    "    except Exception as e:\n",
    "        # log.error(' '.join([s3_output_dir, collection, timestamp, incremental_id]))\n",
    "        log.error(traceback.print_exc())\n",
    "        exit(1)\n",
    "        \n",
    "def download_model(bucket_name,general_path, prefix, job_name): ## example to download any file on S3\n",
    "    bucket_name = 'bucket_name'\n",
    "    general_path = 'dir_path'\n",
    "    file_name = '/output/model.tar.gz'\n",
    "    download_name = prefix + job_name + '.tar.gz'\n",
    "    if not os.path.exists(prefix):\n",
    "        os.makedirs(prefix)\n",
    "    \n",
    "    s3 = boto3.client('s3')\n",
    "    s3.download_file(bucket_name, general_path + prefix + job_name + file_name, download_name)\n",
    "    \n",
    "    return download_name\n",
    "\n",
    "def read_file(bucketname, itemname ): ## instaed of download\n",
    "    s3 = boto3.resource('s3')\n",
    "    obj = s3.Object(bucketname, itemname)\n",
    "    body = obj.get()['Body'].read()\n",
    "    \n",
    "def load_model(cols_input, model_location='model.tar.gz'):\n",
    "    tar = tarfile.open(model_location)\n",
    "    tar.extractall()\n",
    "    tar.close()\n",
    "    model = pkl.load(open('xgboost-model', 'rb'))\n",
    "    map_names = dict(zip(model.feature_names,cols_input))\n",
    "    model.feature_names = list(map_names.values())\n",
    "    return model\n",
    "\n",
    "def get_test_data(bucket_name, general_path,  prefix, job_name):\n",
    "\n",
    "    file_name = 'test/test.csv'\n",
    "    download_name = prefix + 'test.csv'\n",
    "    s3 = boto3.client('s3')\n",
    "    s3.download_file(bucket_name, general_path + prefix_diag + file_name, download_name)\n",
    "    return download_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
